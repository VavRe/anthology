{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import pyreadstat\n",
    "# import yaml\n",
    "# import numpy as np\n",
    "# from scipy.stats import wasserstein_distance\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tqdm import tqdm\n",
    "# import pingouin as pg  # For Cronbach's alpha\n",
    "# import warnings\n",
    "# import glob\n",
    "# warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# # Configuration\n",
    "# USE_SELECTED_QUESTIONS_ONLY = True  # Set to False to use all questions from the survey\n",
    "\n",
    "# # Base directories\n",
    "# # base_dir = '/Users/vavre/Research/TeIAS/impersonation'\n",
    "# base_dir = '/home/erfan/Desktop/actives/bsop'\n",
    "# # /home/erfan/Desktop/actives/bsop/anthology/data/ATPs/W34_Apr18/ATP W34.sav\n",
    "# questions_dir = f'{base_dir}/anthology/data/questions/data/questions'\n",
    "# output_dir = f'{base_dir}/results'\n",
    "\n",
    "# # Create output directory if it doesn't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Hard-coded paths to survey files\n",
    "# survey_files = {\n",
    "#     # '37': f'{base_dir}/ATP/W37_Jul18/ATP W37.sav',\n",
    "#     '34': f'{base_dir}/anthology/data/ATPs/W34_Apr18/ATP W34.sav',\n",
    "#     '99': f'{base_dir}/anthology/data/ATPs/W99_Nov21/ATP W99.sav',\n",
    "#     '92': f'{base_dir}/anthology/data/ATPs/W92_Jul21/ATP W92.sav',\n",
    "#     # Add more waves as needed\n",
    "# }\n",
    "\n",
    "\n",
    "# print(f\"Processing {len(survey_files)} survey waves: {', '.join(sorted(survey_files.keys()))}\")\n",
    "\n",
    "# def process_wave(wave, survey_path, use_selected_questions):\n",
    "#     print(f\"\\n{'='*80}\\nProcessing Wave {wave}\\n{'='*80}\")\n",
    "    \n",
    "#     # Try to load the survey data first to check if it exists\n",
    "#     try:\n",
    "#         df, meta = pyreadstat.read_sav(survey_path)\n",
    "#         print(f\"Loaded survey data with {len(df)} responses and {len(df.columns)} variables\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading survey file {survey_path}: {e}\")\n",
    "#         return None\n",
    "    \n",
    "#     # Find the questions to use\n",
    "#     use_all_questions = False  # Local flag for this wave\n",
    "    \n",
    "#     if use_selected_questions:\n",
    "#         # Look for corresponding questions file\n",
    "#         questions_path = f'{questions_dir}/ATP_W{wave}_demographic_questions.yaml'\n",
    "#         if not os.path.exists(questions_path):\n",
    "#             print(f\"No demographic question file found at {questions_path}\")\n",
    "#             # Look for any question file for this wave\n",
    "#             alt_questions_paths = glob.glob(f'{questions_dir}/ATP_W{wave}_*.yaml')\n",
    "#             if alt_questions_paths:\n",
    "#                 questions_path = alt_questions_paths[0]\n",
    "#                 print(f\"Using alternative question file: {os.path.basename(questions_path)}\")\n",
    "#             else:\n",
    "#                 print(f\"No question files found for wave {wave}. Will use all questions.\")\n",
    "#                 use_all_questions = True\n",
    "        \n",
    "#         # Load selected questions if a file was found\n",
    "#         if not use_all_questions:\n",
    "#             try:\n",
    "#                 with open(questions_path, 'r') as file:\n",
    "#                     selected_questions = list(yaml.safe_load(file)['questions'].keys())\n",
    "#                 print(f\"Loaded {len(selected_questions)} questions from YAML file\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error loading questions from {questions_path}: {e}\")\n",
    "#                 print(\"Will use all questions instead\")\n",
    "#                 use_all_questions = True\n",
    "#     else:\n",
    "#         # Global flag set to use all questions\n",
    "#         use_all_questions = True\n",
    "    \n",
    "#     # If using all questions\n",
    "#     if use_all_questions:\n",
    "#         # Common metadata column prefixes/suffixes to exclude\n",
    "#         exclude_patterns = [\n",
    "#             'WEIGHT', 'ID', 'CASEID', 'PSU', 'STRATUM', 'QKEY', 'DATE', \n",
    "#             'START', 'END', 'DURATION', 'MODE', 'VERSION', 'LANGUAGE',\n",
    "#             'FLAG', 'CONSENT', 'COMPLETE', 'STATUS'\n",
    "#         ]\n",
    "        \n",
    "#         # Filter out metadata columns\n",
    "#         selected_questions = []\n",
    "#         for col in df.columns:\n",
    "#             # Skip if column name contains any excluded pattern\n",
    "#             if any(pattern in col.upper() for pattern in exclude_patterns):\n",
    "#                 continue\n",
    "#             # Only include columns with numeric data (for survey responses)\n",
    "#             if pd.api.types.is_numeric_dtype(df[col]):\n",
    "#                 selected_questions.append(col)\n",
    "        \n",
    "#         print(f\"Using {len(selected_questions)} questions from all available survey variables\")\n",
    "    \n",
    "#     # Ensure we have the correct weight column\n",
    "#     weight_column = f'WEIGHT_W{wave}'\n",
    "#     if weight_column not in df.columns:\n",
    "#         print(f\"Warning: Weight column {weight_column} not found. Available columns that may contain weights:\")\n",
    "#         weight_cols = [col for col in df.columns if 'WEIGHT' in col]\n",
    "#         print(weight_cols)\n",
    "#         if weight_cols:\n",
    "#             weight_column = weight_cols[0]\n",
    "#             print(f\"Using {weight_column} instead.\")\n",
    "#         else:\n",
    "#             print(\"No weight column found. Proceeding without weights.\")\n",
    "#             weight_column = None\n",
    "\n",
    "#     print(f\"Using weight column: {weight_column}\")\n",
    "\n",
    "#     # Function to calculate Frobenius norm between two distribution matrices\n",
    "#     def frobenius_norm(dist1, dist2):\n",
    "#         # Create a common index of all unique values\n",
    "#         all_values = sorted(set(dist1.index) | set(dist2.index))\n",
    "        \n",
    "#         # Create arrays with values for each distribution\n",
    "#         dist1_array = np.array([dist1.get(val, 0) for val in all_values])\n",
    "#         dist2_array = np.array([dist2.get(val, 0) for val in all_values])\n",
    "        \n",
    "#         # Calculate Frobenius norm\n",
    "#         return np.sqrt(np.sum((dist1_array - dist2_array) ** 2))\n",
    "\n",
    "#     # Initialize dictionaries to store results\n",
    "#     question_metrics = {q: {'wd': [], 'frobenius': [], 'cronbach_alpha': []} for q in selected_questions}\n",
    "#     aggregated_metrics = {'wd': {}, 'frobenius': {}, 'cronbach_alpha': {}}\n",
    "\n",
    "#     # Clean non-compliant responses\n",
    "#     df_clean = df.copy()\n",
    "#     for q in selected_questions:\n",
    "#         if q in df_clean.columns:\n",
    "#             # Replace values < 0 (usually error codes)\n",
    "#             df_clean.loc[df_clean[q] < 0, q] = np.nan\n",
    "#             # Replace values > 90 (usually refused/don't know codes in ATP surveys)\n",
    "#             df_clean.loc[df_clean[q] > 90, q] = np.nan\n",
    "\n",
    "#     # Run 100 iterations of random splits\n",
    "#     num_iterations = 100\n",
    "#     for iteration in tqdm(range(num_iterations), desc=f\"Processing Wave {wave}\"):\n",
    "#         # For each question, calculate metrics between two random splits\n",
    "#         for q in selected_questions:\n",
    "#             try:\n",
    "#                 # Skip if question not in dataframe\n",
    "#                 if q not in df_clean.columns:\n",
    "#                     continue\n",
    "                    \n",
    "#                 responses = df_clean[q]\n",
    "                \n",
    "#                 # Skip if too many missing values\n",
    "#                 if responses.isna().mean() > 0.5:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Get the weights (if available)\n",
    "#                 weights = df_clean[weight_column] if weight_column else pd.Series(1, index=df_clean.index)\n",
    "                \n",
    "#                 # Split the indices into two random groups\n",
    "#                 indices = responses.dropna().index\n",
    "#                 if len(indices) < 10:  # Skip questions with very few responses\n",
    "#                     continue\n",
    "                    \n",
    "#                 # Split indices and get corresponding responses and weights\n",
    "#                 train_indices, test_indices = train_test_split(\n",
    "#                     indices, \n",
    "#                     test_size=0.5,\n",
    "#                     random_state=None,  # Different splits each iteration\n",
    "#                     shuffle=True\n",
    "#                 )\n",
    "                \n",
    "#                 r1 = responses.loc[train_indices]\n",
    "#                 r2 = responses.loc[test_indices]\n",
    "#                 w1 = weights.loc[train_indices]\n",
    "#                 w2 = weights.loc[test_indices]\n",
    "                \n",
    "#                 # Calculate weighted distributions\n",
    "#                 # First, get unique response values (excluding NaN)\n",
    "#                 unique_vals = pd.Series(list(set(r1.dropna()) | set(r2.dropna())))\n",
    "                \n",
    "#                 # Create weighted distributions\n",
    "#                 r1_dist = {}\n",
    "#                 r2_dist = {}\n",
    "                \n",
    "#                 for val in unique_vals:\n",
    "#                     # Sum weights for each response value\n",
    "#                     r1_mask = (r1 == val)\n",
    "#                     r2_mask = (r2 == val)\n",
    "                    \n",
    "#                     if r1_mask.any():\n",
    "#                         r1_dist[val] = w1[r1_mask].sum()\n",
    "#                     else:\n",
    "#                         r1_dist[val] = 0\n",
    "                        \n",
    "#                     if r2_mask.any():\n",
    "#                         r2_dist[val] = w2[r2_mask].sum()\n",
    "#                     else:\n",
    "#                         r2_dist[val] = 0\n",
    "                \n",
    "#                 # Convert to pandas Series\n",
    "#                 r1_dist = pd.Series(r1_dist)\n",
    "#                 r2_dist = pd.Series(r2_dist)\n",
    "                \n",
    "#                 # Normalize to get probability distributions\n",
    "#                 if r1_dist.sum() > 0:\n",
    "#                     r1_dist = r1_dist / r1_dist.sum()\n",
    "#                 if r2_dist.sum() > 0:\n",
    "#                     r2_dist = r2_dist / r2_dist.sum()\n",
    "                \n",
    "#                 # Check if values are numeric for Wasserstein distance\n",
    "#                 all_values = sorted(set(r1_dist.index) | set(r2_dist.index))\n",
    "#                 numeric_values = []\n",
    "#                 is_numeric = True\n",
    "                \n",
    "#                 for x in all_values:\n",
    "#                     if pd.isna(x):\n",
    "#                         continue\n",
    "#                     elif isinstance(x, (int, float)):\n",
    "#                         numeric_values.append(x)\n",
    "#                     else:\n",
    "#                         try:\n",
    "#                             # Try converting to numeric\n",
    "#                             num_val = float(x)\n",
    "#                             numeric_values.append(num_val)\n",
    "#                         except:\n",
    "#                             is_numeric = False\n",
    "#                             break\n",
    "                \n",
    "#                 # Calculate Wasserstein Distance if values are numeric\n",
    "#                 if is_numeric and len(numeric_values) > 1:\n",
    "#                     try:\n",
    "#                         # Get probability arrays for numeric values\n",
    "#                         p1 = np.array([r1_dist.get(val, 0) for val in numeric_values])\n",
    "#                         p2 = np.array([r2_dist.get(val, 0) for val in numeric_values])\n",
    "                        \n",
    "#                         # Normalize if needed\n",
    "#                         if p1.sum() > 0:\n",
    "#                             p1 = p1 / p1.sum()\n",
    "#                         if p2.sum() > 0:\n",
    "#                             p2 = p2 / p2.sum()\n",
    "                        \n",
    "#                         wd = wasserstein_distance(numeric_values, numeric_values, p1, p2)\n",
    "#                         question_metrics[q]['wd'].append(wd)\n",
    "#                     except Exception as e:\n",
    "#                         # Skip silently\n",
    "#                         pass\n",
    "                \n",
    "#                 # Calculate Frobenius Norm\n",
    "#                 try:\n",
    "#                     frobenius = frobenius_norm(r1_dist, r2_dist)\n",
    "#                     question_metrics[q]['frobenius'].append(frobenius)\n",
    "#                 except Exception as e:\n",
    "#                     # Skip silently\n",
    "#                     pass\n",
    "                \n",
    "#                 # Prepare data for Cronbach's Alpha\n",
    "#                 try:\n",
    "#                     # For Cronbach's Alpha, we need original responses, not distributions\n",
    "#                     # Create new indices to avoid duplicates\n",
    "#                     r1_reset = r1.reset_index(drop=True)\n",
    "#                     r2_reset = r2.reset_index(drop=True)\n",
    "                    \n",
    "#                     # Create a dataframe with responses and group indicators\n",
    "#                     df_alpha = pd.DataFrame({\n",
    "#                         'response': pd.concat([r1_reset, r2_reset]),\n",
    "#                         'group': pd.concat([pd.Series(['g1']*len(r1_reset)), pd.Series(['g2']*len(r2_reset))])\n",
    "#                     })\n",
    "                    \n",
    "#                     # Only proceed if responses are numeric\n",
    "#                     if pd.to_numeric(df_alpha['response'], errors='coerce').notna().all():\n",
    "#                         # Create pivot table for Cronbach's alpha calculation\n",
    "#                         pivot_data = pd.pivot_table(\n",
    "#                             df_alpha, \n",
    "#                             values='response', \n",
    "#                             index=df_alpha.index, \n",
    "#                             columns='group', \n",
    "#                             aggfunc='first'\n",
    "#                         )\n",
    "                        \n",
    "#                         # Calculate Cronbach's Alpha\n",
    "#                         if pivot_data.shape[1] >= 2:  # Need at least 2 columns\n",
    "#                             alpha_result = pg.cronbach_alpha(pivot_data)\n",
    "#                             alpha = alpha_result[0]  # Extract only the alpha value\n",
    "#                             question_metrics[q]['cronbach_alpha'].append(alpha)\n",
    "#                 except Exception as e:\n",
    "#                     # Skip if Cronbach's alpha calculation fails\n",
    "#                     pass\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 # Skip silently\n",
    "#                 continue\n",
    "\n",
    "#     # Calculate average metrics for each question\n",
    "#     for q in selected_questions:\n",
    "#         if question_metrics[q]['wd']:\n",
    "#             aggregated_metrics['wd'][q] = np.mean(question_metrics[q]['wd'])\n",
    "        \n",
    "#         if question_metrics[q]['frobenius']:\n",
    "#             aggregated_metrics['frobenius'][q] = np.mean(question_metrics[q]['frobenius'])\n",
    "        \n",
    "#         if question_metrics[q]['cronbach_alpha']:\n",
    "#             aggregated_metrics['cronbach_alpha'][q] = np.mean(question_metrics[q]['cronbach_alpha'])\n",
    "\n",
    "#     # Print results\n",
    "#     print(f\"\\nHuman Baseline Metrics for Wave {wave}:\")\n",
    "#     print(\"-\" * 80)\n",
    "#     print(\"Average Wasserstein Distance across all questions:\", \n",
    "#           np.mean(list(aggregated_metrics['wd'].values())) if aggregated_metrics['wd'] else \"N/A\")\n",
    "#     print(\"Average Frobenius Norm across all questions:\", \n",
    "#           np.mean(list(aggregated_metrics['frobenius'].values())) if aggregated_metrics['frobenius'] else \"N/A\")\n",
    "#     print(\"Average Cronbach's Alpha across all questions:\", \n",
    "#           np.mean(list(aggregated_metrics['cronbach_alpha'].values())) if aggregated_metrics['cronbach_alpha'] else \"N/A\")\n",
    "#     print(\"-\" * 80)\n",
    "\n",
    "#     # Calculate how many questions had valid metrics\n",
    "#     valid_wd = len(aggregated_metrics['wd'])\n",
    "#     valid_fro = len(aggregated_metrics['frobenius'])\n",
    "#     valid_alpha = len(aggregated_metrics['cronbach_alpha'])\n",
    "#     print(f\"Questions with valid metrics (out of {len(selected_questions)}):\")\n",
    "#     print(f\"  Wasserstein Distance: {valid_wd} ({valid_wd/len(selected_questions)*100:.1f}%)\")\n",
    "#     print(f\"  Frobenius Norm: {valid_fro} ({valid_fro/len(selected_questions)*100:.1f}%)\")\n",
    "#     print(f\"  Cronbach's Alpha: {valid_alpha} ({valid_alpha/len(selected_questions)*100:.1f}%)\")\n",
    "\n",
    "#     # Prepare results dataframe\n",
    "#     results_df = pd.DataFrame({\n",
    "#         'question': selected_questions,\n",
    "#         'wasserstein_distance': [aggregated_metrics['wd'].get(q, np.nan) for q in selected_questions],\n",
    "#         'frobenius_norm': [aggregated_metrics['frobenius'].get(q, np.nan) for q in selected_questions],\n",
    "#         'cronbach_alpha': [aggregated_metrics['cronbach_alpha'].get(q, np.nan) for q in selected_questions]\n",
    "#     })\n",
    "\n",
    "#     # Add a row with overall averages\n",
    "#     overall_row = pd.DataFrame({\n",
    "#         'question': ['OVERALL_AVERAGE'],\n",
    "#         'wasserstein_distance': [np.mean(list(aggregated_metrics['wd'].values())) if aggregated_metrics['wd'] else np.nan],\n",
    "#         'frobenius_norm': [np.mean(list(aggregated_metrics['frobenius'].values())) if aggregated_metrics['frobenius'] else np.nan],\n",
    "#         'cronbach_alpha': [np.mean(list(aggregated_metrics['cronbach_alpha'].values())) if aggregated_metrics['cronbach_alpha'] else np.nan]\n",
    "#     })\n",
    "\n",
    "#     # Append the overall row\n",
    "#     results_df = pd.concat([results_df, overall_row], ignore_index=True)\n",
    "\n",
    "#     # Create output filename with flag indicator\n",
    "#     question_source = \"selected\" if not use_all_questions else \"all\"\n",
    "#     output_path = f'{output_dir}/human_baseline_weighted_W{wave}_{question_source}_questions.csv'\n",
    "#     results_df.to_csv(output_path, index=False)\n",
    "#     print(f\"Results saved to: {output_path}\")\n",
    "    \n",
    "#     return {\n",
    "#         'wave': wave,\n",
    "#         'question_source': question_source,\n",
    "#         'num_questions': len(selected_questions),\n",
    "#         'valid_wd': valid_wd,\n",
    "#         'valid_frobenius': valid_fro,\n",
    "#         'valid_alpha': valid_alpha,\n",
    "#         'avg_wd': np.mean(list(aggregated_metrics['wd'].values())) if aggregated_metrics['wd'] else np.nan,\n",
    "#         'avg_frobenius': np.mean(list(aggregated_metrics['frobenius'].values())) if aggregated_metrics['frobenius'] else np.nan,\n",
    "#         'avg_alpha': np.mean(list(aggregated_metrics['cronbach_alpha'].values())) if aggregated_metrics['cronbach_alpha'] else np.nan\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Process all waves and collect summary results\n",
    "# summary_results = []\n",
    "# for wave, survey_path in survey_files.items():\n",
    "#     result = process_wave(wave, survey_path, USE_SELECTED_QUESTIONS_ONLY)\n",
    "#     if result:\n",
    "#         summary_results.append(result)\n",
    "\n",
    "# # Create and save summary dataframe\n",
    "# if summary_results:\n",
    "#     question_source = \"selected\" if USE_SELECTED_QUESTIONS_ONLY else \"all\"\n",
    "#     summary_df = pd.DataFrame(summary_results)\n",
    "#     summary_output_path = f'{output_dir}/human_baseline_summary_{question_source}_questions.csv'\n",
    "#     summary_df.to_csv(summary_output_path, index=False)\n",
    "    \n",
    "#     print(f\"\\n{'='*80}\\nSummary of All Waves\\n{'='*80}\")\n",
    "#     print(summary_df)\n",
    "#     print(f\"\\nSummary saved to: {summary_output_path}\")\n",
    "# else:\n",
    "#     print(\"\\nNo results were generated for any wave.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat, json, yaml\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from pingouin import cronbach_alpha\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "from minilib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "wave = '92'\n",
      "len(selected_questions) = 7\n",
      "len(df) = 10916\n",
      "WD: 0.083\n",
      "Fro: nan\n",
      "α: 0.54\n",
      "========================================\n",
      "wave = '34'\n",
      "len(selected_questions) = 8\n",
      "len(df) = 2537\n",
      "WD: 0.05\n",
      "Fro: 0.425\n",
      "α: 0.785\n",
      "========================================\n",
      "wave = '99'\n",
      "len(selected_questions) = 6\n",
      "len(df) = 10260\n",
      "WD: 0.09\n",
      "Fro: 0.313\n",
      "α: 0.829\n"
     ]
    }
   ],
   "source": [
    "survey_paths = [\n",
    "    '../data/ATPs/W92_Jul21/ATP W92.sav',\n",
    "    '../data/ATPs/W34_Apr18/ATP W34.sav',\n",
    "    '../data/ATPs/W99_Nov21/ATP W99.sav',\n",
    "]\n",
    "\n",
    "\n",
    "for survey_path in survey_paths:\n",
    "    print(40*'=')\n",
    "    wave = survey_path.split()[-1].split('.')[0][1:]\n",
    "    weight_col = f'WEIGHT_W{wave}'\n",
    "    print(f'{wave = }')\n",
    "\n",
    "    selected_questions_path = f'/home/erfan/Desktop/actives/bsop/anthology/configs/surveys/questionnaire/ATP/ATP_W{wave}_questions.yaml'\n",
    "\n",
    "    # [STEP] get_human_data()\n",
    "    df, meta = pyreadstat.read_sav(survey_path)\n",
    "\n",
    "    # with open(selected_questions_path, 'r') as file:\n",
    "    #     selected_questions = json.load(file)\n",
    "        \n",
    "    with open(selected_questions_path, 'r') as file:\n",
    "        selected_questions = list(yaml.safe_load(file)['questions'].keys())\n",
    "        \n",
    "        \n",
    "    # selected_questions = [col for col in meta.column_names if col.endswith(f'_W{wave}') and col not in [f'INTERVIEW_START_W{wave}', f'INTERVIEW_END_W{wave}', f'DEVICE_TYPE_W{wave}', f'LANG_W{wave}', f'FORM_W{wave}']]\n",
    "    print(f'{len(selected_questions) = }')\n",
    "    print(f'{len(df) = }')\n",
    "    \n",
    "    df = remove_non_compliant_responses(df, selected_questions)\n",
    "\n",
    "    # for _ in tqdm(range(100)):\n",
    "    df_comp = df.sample(2500, replace=False)\n",
    "    wd = distance_lower_bound(df_comp, selected_questions, weight_column=weight_col, dist_type='EMD', num_iter=100)['average EMD'].item()\n",
    "    fro = matrix_dist_lower_bound(df_comp, selected_questions, wave=wave, dist_type=\"Frobenius\", num_iter=100).item()\n",
    "    ca = cronbach_alpha(df_comp[selected_questions])[0].item()\n",
    "        \n",
    "    print(f'WD: {round(wd, 3)}')\n",
    "    print(f'Fro: {round(fro, 3)}')\n",
    "    print(f'α: {round(ca, 3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POSNEGAI_a_W99': 0.08919909328120144,\n",
       " 'POSNEGAI_b_W99': 0.07687652650343586,\n",
       " 'POSNEGAI_c_W99': 0.08928317061439697,\n",
       " 'POSNEGAI_d_W99': 0.09305834441006074,\n",
       " 'POSNEGAI_e_W99': 0.084745965747097,\n",
       " 'POSNEGAI_f_W99': 0.09304918205766328,\n",
       " 'average EMD': np.float64(0.08770204710230922)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp_X = df_comp[selected_questions].values\n",
    "df_comp_w = df_comp[weight_col].values\n",
    "\n",
    "_, human_weighted_cov = get_weighted_cov(df_comp_X, df_comp_w, corr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.14536221536353883)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrix_distance(\n",
    "                human_weighted_cov,\n",
    "                df_comp[selected_questions].corr().values,\n",
    "                dist_type=\"Frobenius\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = remove_non_compliant_responses(df, selected_questions)\n",
    "\n",
    "# qwd = {k: [] for k in selected_questions}\n",
    "\n",
    "# for _ in tqdm(range(100)):\n",
    "#     df_comp = df.sample(2500, replace=False)\n",
    "#     i1, i2 = train_test_split(\n",
    "#         df_comp.index,\n",
    "#         test_size=0.5,\n",
    "#         random_state=None, # Use None for different splits each iteration\n",
    "#         shuffle=True\n",
    "#     )\n",
    "        \n",
    "#     qd1, _ = get_human_response(df_comp.loc[i1], selected_questions, df_comp[f'WEIGHT_W{wave}'].loc[i1])\n",
    "#     qd2, _ = get_human_response(df_comp.loc[i2], selected_questions, df_comp[f'WEIGHT_W{wave}'].loc[i2])\n",
    "    \n",
    "#     for q in selected_questions:\n",
    "#         d1, d2 = qd1[q], qd2[q]\n",
    "#         assert len(d1) == len(d2), 'Dists have different likerts'\n",
    "#         unique_responses = sorted(list(df_comp[q].value_counts(normalize=True).index))\n",
    "#         assert len(unique_responses) == len(d1), 'Dists miss likerts'\n",
    "        \n",
    "#         wd = wasserstein_distance(unique_responses, unique_responses, d1, d2)\n",
    "#         qwd[q].append(wd.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anthology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
